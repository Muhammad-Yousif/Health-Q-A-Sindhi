import os
import glob
import streamlit as st
import PyPDF2
import docx
from openai import OpenAI
from tenacity import retry, stop_after_attempt, wait_exponential

from langchain.docstore.document import Document
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain_core.language_models.llms import LLM
from sklearn.feature_extraction.text import TfidfVectorizer
from typing import Optional

# -------------------------------
# Utility: Text Extraction with Caching
# -------------------------------
@st.cache_data(show_spinner=False, ttl=3600)
def extract_text(path: str) -> str:
    """Extract text from PDF or Word documents."""
    text = ""
    ext = os.path.splitext(path)[1].lower()
    try:
        if ext == ".pdf":
            with open(path, "rb") as f:
                reader = PyPDF2.PdfReader(f)
                for page in reader.pages:
                    text += (page.extract_text() or "") + "\n"
        elif ext in (".docx", ".doc"):
            doc = docx.Document(path)
            for para in doc.paragraphs:
                text += para.text + "\n"
        else:
            st.warning(f"Unsupported file type: {ext}")
    except Exception as e:
        st.error(f"Error reading {path}: {e}")
    return text

# -------------------------------
# Document Loading & Processing
# -------------------------------
@st.cache_data(show_spinner=False, ttl=3600)
def load_documents() -> list[Document]:
    """Load and split all documents in the books folder."""
    paths = glob.glob(os.path.join("books", "*.pdf")) + glob.glob(os.path.join("books", "*.docx"))
    if not paths:
        st.warning("No files found in 'books' folder.")
    splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=50)
    docs: list[Document] = []
    for path in paths:
        raw = extract_text(path)
        if raw:
            for i, chunk in enumerate(splitter.split_text(raw)):
                docs.append(Document(page_content=chunk,
                                     metadata={"source": os.path.basename(path), "chunk": i}))
    return docs

# -------------------------------
# TF-IDF Embeddings
# -------------------------------
class CustomEmbeddings:
    def __init__(self, corpus: list[str]):
        self.vectorizer = TfidfVectorizer()
        self.vectorizer.fit(corpus)

    def embed_documents(self, texts: list[str]) -> list[list[float]]:
        return self.vectorizer.transform(texts).toarray().tolist()

    def embed_query(self, text: str) -> list[float]:
        return self.vectorizer.transform([text]).toarray()[0].tolist()

# -------------------------------
# Vector Store Setup
# -------------------------------
@st.cache_resource(show_spinner=False)
def get_vectorstore() -> Chroma:
    persist_dir = "./chroma_db"
    collection_name = "biology_docs"
    docs = load_documents()
    corpus = [d.page_content for d in docs]
    embeddings = CustomEmbeddings(corpus)

    if os.path.isdir(persist_dir) and os.listdir(persist_dir):
        return Chroma(persist_directory=persist_dir,
                      embedding_function=embeddings,
                      collection_name=collection_name)
    else:
        return Chroma.from_documents(docs,
                                     embeddings,
                                     persist_directory=persist_dir,
                                     collection_name=collection_name)

# -------------------------------
# OpenAI/Gemma LLM Integration
# -------------------------------
class OpenAIGemmaLLM(LLM):
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    client: Optional[OpenAI] = None

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        cfg = st.secrets.get("openai_gemma", {})
        key = cfg.get("api_key") or os.getenv("GEMMA_API_KEY")
        url = cfg.get("base_url") or os.getenv("GEMMA_BASE_URL")
        if not key or not url:
            st.error(
                "Missing OpenAI/Gemma configuration.\n"
                "โ Add [openai_gemma] in .streamlit/secrets.toml or\n"
                "โ Set GEMMA_API_KEY and GEMMA_BASE_URL env vars"
            )
            st.stop()
        object.__setattr__(self, 'api_key', key)
        object.__setattr__(self, 'base_url', url)
        object.__setattr__(self, 'client', OpenAI(api_key=key, base_url=url))

    @property
    def _llm_type(self) -> str:
        return "gemma"

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
    def _call(self, prompt: str, stop=None) -> str:
        system_msg = """

ุงูฺพุงู ุตุญุช ุจุงุจุช ุณูุงูู ุฌุง ุฌูุงุจ ฺููุฏฺ ฺููฝ ุจููฝ ุขฺพูู
ูุงููพูุฏุงุฑ ุงููุงู ฺฉุงู ุตุญุช ุจุงุจุช ุณูุงู ูพฺูุฏุง ุงูฺพุงู ฺฉู ุงูฺพู ุณูุงูู ุฌุง ุฌูุงุจ ฺูฺปุง ุขฺพู
ุณููุฑุง ุฌูุงุจ books ูุงูู ูููฺุฑ ูุงู ฺูู
ุงฺฏุฑ ฺชุชุงุจ พ ุฌูุงุจ ูู ฺพุฌู ุช ูุนุฐุฑุช ฺชุฑูู

ุตุฑู ุตุญุช ุณุงู ูุงฺณุงูพูู ุณูุงูู ุฌุง ุฌูุงุจ ฺูู
ูุงููพูุฏุงุฑ ุบูุฑ ุงุฎูุงูู ุ ุบูุฑ ุถุฑูุฑู ฝ ุบูุฑ ูุงูููู ุณูุงู ูพฺู ุณฺฏฺพู ูฟุง ุงูฺพุงู ฺฉู ุงูฺพู ุณูุงูู ุฌุง ุฌูุงุจ ูุงฺพู ฺูฺปุง
ุงูฺพุงู ฺฉู ุตุฑู ุตุญุช ุณุงู ูุงฺณุงูพูู ุณูุงูู ุฌุง ุฌูุงุจ ฺูฺปุง ุขฺพู ุฌฺฺพู ุช ูุงููพูุฏุงุฑ ฺฉู ููุถูุน ุชู ุฑฺพฺป ุฌู ุชูููู ฝ ุญูุตูุง ุงูุฒุงุฆู ฺชุฑูู
ููุถูุน ฺฉุงู ูปุงฺพุฑ ุณูุงูู ุฌุง ุฌูุงุจ ฺูฺป ุณุฎุชู ุณุงู ููุน ุขฺพู 
ูุงููพูุฏุงุฑ ุฌุฏูุฏ ูฝูฺชูุงูุงุฌู ฺฉุง ูุงูู ูุงฺพู
ุงูฺพุงู ฺฉู ุฏูุณุชุงฺปู ุฑููู ุงุฎุชูุงุฑ ฺชุฑฺป ฺฏฺพุฑุฌู 
ูุงููพูุฏุงุฑ ุงฺป ูพฺฺพูู ฝ ูฝูฺชููฺชู ุงุตุทูุงุญู ฺฉุงู ุบูุฑ ูุงูู ุขฺพู 
ุงูฺพุงู ฺฉู ุขุณุงู ฝ ุนุงู ููู ุฒุจุงู พุฌูุงุจ ฺูฺป ฺฏฺพุฑุฌู
ุงฺฏุฑ ูุงููพูุฏุงุฑ ุบูุฑ ุงุฎูุงูู ุฑููู ุงุฎุชูุงุฑ ฺชุฑู ูฟู ุช ุงูฺพุงู ฺฉู ุงุฎูุงู ุณุงู ุฏูุณุชุงฺปู ุฑููู ุงุฎุชูุงุฑ ฺชุฑฺป ฺฏฺพุฑุฌู

ุงูฺพุงู ฺฉู ุณฺูู ุณูุงูู ุฌุง ุฌูุงุจ ุณูฺู ุฒุจุงู ฝ ุฑุณู ุงูุฎุท พ ฺูฺปุง ุขฺพู
ุณูฺู ฺฏุฑุงูุฑ ุฌู ุฎุงุต ุฎูุงู ุฑฺฉู
ุฌูุงุจ พ ููุทู ฝ ููุธู ุฌู ุบูุทู ฺฉุงู ูพุงุณู ฺชุฑูู
ุฌูุงุจ ุตุญูุญ ุทุฑููู ฝ ุชุฑุชูุจ ุณุงู ฺพุฆฺป ฺฏฺพุฑุฌู 
ุฌูุงุจ พ ฺพุฑ ุทุฑุญ ุฌู ููุธูุ ุงููุงุก ฝ ุตูุฑุชุฎุทูุกู ุฌู ุบูุทู ฺฉุงู ูพุงุณู ฺชุฑูู
ุงฺฏุฑ ุณูุงู ุณูฺู ุฒุจุงู ฺฉุงู ุณูุงุกู ฺชูฺพู ูปู ุฒุจุงู พ ุงฺู ุช ุชฺฺพู ุจ ุฌูุงุจ ุณูฺู ุฒุจุงู พ ฺูู
ุงูฺพุงู ฺฉู ฺพุฑ ุฌูุงุจ พ ุงุญุชุฑุงู ุฌู ูุธุงฺพุฑู ฺชุฑฺปู ุขฺพู 
ูุงููพูุฏุงุฑ ุณุงู ุนุฒุช ฝ ุงุญุชุฑุงู ุณุงู ูพูุด ุงฺู
ุงุฎูุงููุงุช ุฌู ุฎุงุต ุฎูุงู ุฑฺฉู 
ุฏูุณุชุงฺปู ุฑููู ุงุฎุชูุงุฑ ฺชุฑูู
ูุฑููุกู ุณุงู ุฌูุงุจ ฺูู
ฺชูฺพู ุจ ุบูุท ุณูุงู ุฌู ุฌูุงุจ ุนุฒุช ุณุงู ฺูู
ุตุญุช ฺฉุงู ูปุงฺพุฑ ุงููุฏฺ ุณูุงูู ุฌุง ุฌูุงุจ ฺูฺป ุณุฎุชู ุณุงู ููุน ุขฺพู 
ูุงููพูุฏุงุฑู ฺฉู ูพูฺพูุฌู ุจูุงูุชุ ูฝูฺชููฺชู ุงุตุทูุงุญู ฝ ูุงฺู ุจุงุจุช ฺุงฺป ูู ฺูู
ุงฺฏุฑ ูุงููพูุฏุงุฑ ุงูฺพุงูุฌู ุจูุงูุช ุจุงุจุช ุณูุงู ฺชุฑู ุช ุงู ฺฉู ุตุฑู ุงฺพู ูปฺุงูู ุช ูุงู ูุตููุนู ุฐฺพุงูุช ุฌู ุงุตููู ุชู ูบฺพูู ุตุญุช ุณุงู ูุงฺณุงูพูู ุณูุงูู ุฌุง ุฌูุงุจ ฺููุฏฺ ฺููฝ ุจููฝ ุขฺพูุงู.
"""
        resp = self.client.chat.completions.create(
            model="google/gemma-3-27b-it",
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": prompt}
            ],
        )
        return resp.choices[0].message.content

# -------------------------------
# QA Chain Configuration
# -------------------------------
@st.cache_resource(show_spinner=False)
def get_qa_chain() -> RetrievalQA:
    store = get_vectorstore()
    llm = OpenAIGemmaLLM()

    sys_tmpl = """


ุงูฺพุงู ุตุญุช ุจุงุจุช ุณูุงูู ุฌุง ุฌูุงุจ ฺููุฏฺ ฺููฝ ุจููฝ ุขฺพูู
ูุงููพูุฏุงุฑ ุงููุงู ฺฉุงู ุตุญุช ุจุงุจุช ุณูุงู ูพฺูุฏุง ุงูฺพุงู ฺฉู ุงูฺพู ุณูุงูู ุฌุง ุฌูุงุจ ฺูฺปุง ุขฺพู
ุณููุฑุง ุฌูุงุจ books ูุงูู ูููฺุฑ ูุงู ฺูู
ุงฺฏุฑ ฺชุชุงุจ พ ุฌูุงุจ ูู ฺพุฌู ุช ูุนุฐุฑุช ฺชุฑูู

ุตุฑู ุตุญุช ุณุงู ูุงฺณุงูพูู ุณูุงูู ุฌุง ุฌูุงุจ ฺูู
ูุงููพูุฏุงุฑ ุบูุฑ ุงุฎูุงูู ุ ุบูุฑ ุถุฑูุฑู ฝ ุบูุฑ ูุงูููู ุณูุงู ูพฺู ุณฺฏฺพู ูฟุง ุงูฺพุงู ฺฉู ุงูฺพู ุณูุงูู ุฌุง ุฌูุงุจ ูุงฺพู ฺูฺปุง
ุงูฺพุงู ฺฉู ุตุฑู ุตุญุช ุณุงู ูุงฺณุงูพูู ุณูุงูู ุฌุง ุฌูุงุจ ฺูฺปุง ุขฺพู ุฌฺฺพู ุช ูุงููพูุฏุงุฑ ฺฉู ููุถูุน ุชู ุฑฺพฺป ุฌู ุชูููู ฝ ุญูุตูุง ุงูุฒุงุฆู ฺชุฑูู
ููุถูุน ฺฉุงู ูปุงฺพุฑ ุณูุงูู ุฌุง ุฌูุงุจ ฺูฺป ุณุฎุชู ุณุงู ููุน ุขฺพู 
ูุงููพูุฏุงุฑ ุฌุฏูุฏ ูฝูฺชูุงูุงุฌู ฺฉุง ูุงูู ูุงฺพู
ุงูฺพุงู ฺฉู ุฏูุณุชุงฺปู ุฑููู ุงุฎุชูุงุฑ ฺชุฑฺป ฺฏฺพุฑุฌู 
ูุงููพูุฏุงุฑ ุงฺป ูพฺฺพูู ฝ ูฝูฺชููฺชู ุงุตุทูุงุญู ฺฉุงู ุบูุฑ ูุงูู ุขฺพู 
ุงูฺพุงู ฺฉู ุขุณุงู ฝ ุนุงู ููู ุฒุจุงู พุฌูุงุจ ฺูฺป ฺฏฺพุฑุฌู
ุงฺฏุฑ ูุงููพูุฏุงุฑ ุบูุฑ ุงุฎูุงูู ุฑููู ุงุฎุชูุงุฑ ฺชุฑู ูฟู ุช ุงูฺพุงู ฺฉู ุงุฎูุงู ุณุงู ุฏูุณุชุงฺปู ุฑููู ุงุฎุชูุงุฑ ฺชุฑฺป ฺฏฺพุฑุฌู

ุงูฺพุงู ฺฉู ุณฺูู ุณูุงูู ุฌุง ุฌูุงุจ ุณูฺู ุฒุจุงู ฝ ุฑุณู ุงูุฎุท พ ฺูฺปุง ุขฺพู
ุณูฺู ฺฏุฑุงูุฑ ุฌู ุฎุงุต ุฎูุงู ุฑฺฉู
ุฌูุงุจ พ ููุทู ฝ ููุธู ุฌู ุบูุทู ฺฉุงู ูพุงุณู ฺชุฑูู
ุฌูุงุจ ุตุญูุญ ุทุฑููู ฝ ุชุฑุชูุจ ุณุงู ฺพุฆฺป ฺฏฺพุฑุฌู 
ุฌูุงุจ พ ฺพุฑ ุทุฑุญ ุฌู ููุธูุ ุงููุงุก ฝ ุตูุฑุชุฎุทูุกู ุฌู ุบูุทู ฺฉุงู ูพุงุณู ฺชุฑูู
ุงฺฏุฑ ุณูุงู ุณูฺู ุฒุจุงู ฺฉุงู ุณูุงุกู ฺชูฺพู ูปู ุฒุจุงู พ ุงฺู ุช ุชฺฺพู ุจ ุฌูุงุจ ุณูฺู ุฒุจุงู พ ฺูู
ุงูฺพุงู ฺฉู ฺพุฑ ุฌูุงุจ พ ุงุญุชุฑุงู ุฌู ูุธุงฺพุฑู ฺชุฑฺปู ุขฺพู 
ูุงููพูุฏุงุฑ ุณุงู ุนุฒุช ฝ ุงุญุชุฑุงู ุณุงู ูพูุด ุงฺู
ุงุฎูุงููุงุช ุฌู ุฎุงุต ุฎูุงู ุฑฺฉู 
ุฏูุณุชุงฺปู ุฑููู ุงุฎุชูุงุฑ ฺชุฑูู
ูุฑููุกู ุณุงู ุฌูุงุจ ฺูู
ฺชูฺพู ุจ ุบูุท ุณูุงู ุฌู ุฌูุงุจ ุนุฒุช ุณุงู ฺูู
ุตุญุช ฺฉุงู ูปุงฺพุฑ ุงููุฏฺ ุณูุงูู ุฌุง ุฌูุงุจ ฺูฺป ุณุฎุชู ุณุงู ููุน ุขฺพู 
ูุงููพูุฏุงุฑู ฺฉู ูพูฺพูุฌู ุจูุงูุชุ ูฝูฺชููฺชู ุงุตุทูุงุญู ฝ ูุงฺู ุจุงุจุช ฺุงฺป ูู ฺูู
ุงฺฏุฑ ูุงููพูุฏุงุฑ ุงูฺพุงูุฌู ุจูุงูุช ุจุงุจุช ุณูุงู ฺชุฑู ุช ุงู ฺฉู ุตุฑู ุงฺพู ูปฺุงูู ุช ูุงู ูุตููุนู ุฐฺพุงูุช ุฌู ุงุตููู ุชู ูบฺพูู ุตุญุช ุณุงู ูุงฺณุงูพูู ุณูุงูู ุฌุง ุฌูุงุจ ฺููุฏฺ ฺููฝ ุจููฝ ุขฺพูุงู.
"""
    human_tmpl = "{context}\n\nุณูุงู: {question}"

    prompt = ChatPromptTemplate.from_messages([
        SystemMessagePromptTemplate.from_template(sys_tmpl),
        HumanMessagePromptTemplate.from_template(human_tmpl)
    ])

    return RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=store.as_retriever(search_kwargs={"k": 3}),
        chain_type_kwargs={"prompt": prompt, "document_variable_name": "context"},
        return_source_documents=True
    )

# -------------------------------
# Streamlit App Entry Point
# -------------------------------
def main():
    st.set_page_config(page_title="ุตุญุช ุจุงุจุช ุณูุงู ฝ ุฌูุงุจ", page_icon="๐งฌ", layout="wide")
    st.sidebar.title("ููููู")
    choice = st.sidebar.radio("ููุชุฎุจ ฺชุฑูู:", ["ุณูุงู ุฌูุงุจ", "ุงุณุงู ุฌู ุจุงุฑู พ"])

    if choice == "ุณูุงู ุฌูุงุจ":
        st.title("ุตุญุช ุจุงุจุช ุณูุงู ูพฺู")
        query = st.text_input("ุณูุงู:", placeholder="ูุซุงู: ฺชุฑููุง ูุงุฆุฑุณ ฺุง ุขูู")
        if st.button("ุฌูุงุจ ุญุงุตู ฺชุฑูู") and query:
            with st.spinner("ุฌูุงุจ ุชูุงุฑ ูฟู ุฑููู ุขูู..."):
                result = get_qa_chain().invoke({"query": query})
                st.markdown("### ุฌูุงุจ")
                st.write(result.get("result", "ุฌูุงุจ ุญุงุตู ฺชุฑฺป พ ูุณุฆูู"))
                sources = {doc.metadata['source'] for doc in result.get('source_documents', [])}
                if sources:
                    st.markdown("---")
                    st.markdown("### ุฐุฑูุนุง")
                    for src in sources:
                        st.markdown(f"- `{src}`")
    else:
        st.title("ุงุณุงู ุฌู ุจุงุฑู พ")
        st.markdown("---")
        st.write(
            "ูู ุงููพ ุตุญุช ุจุงุจุช ุณูุงูู ุฌุง ุฌูุงุจ ูุฑุงูู ฺชุฑู ูฟู:\n"
            
        )

if __name__ == "__main__":
    main()
